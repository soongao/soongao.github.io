---
title: Paper Reading SAM
# date: YYYY-MM-DD HH:MM:SS +/-TTTT
categories: [papers]
tags: [papers, sam]     # TAG names should always be lowercase
# toc: false
---

# SAM
- Segment Anything
- https://segment-anything.com


## Code Analysis
### image encoder
- 传统的ViT

### prompt encoder
#### Sparse Embedding
![Sparse Embedding](/assets/img/papers-files/sam/sparse_embedding.png)
- point
  - 每个point坐标[x,y], 还有对应这个point的label[-1(忽略),0(背景),1(前景)]
  - label为-1的点用作pad, 当没有box时, 添加pad
- box
  - 每个box是两个点[x0,y0,x1,y1], 转换为[n,2,2]后, 就等于把box转换成点来处理
- text
  - 官方未实现, 第三方实现(sam改进, 变体)
  - lang sam (Grounding DINO + clip)
  - fastsam (YOLOv8-seg + clip)
#### Dense Embedding
![Dense Embedding](/assets/img/papers-files/sam/dense_embedding.png)
- mask
  - 没指定mask时, 使用no_mask_embed
  - 指定mask, mask下采样成image size
  - mask和image相加作为decoder的输入

### mask decoder
![mask decoder](/assets/img/papers-files/sam/mask_decoder.png)
![mask decoder](/assets/img/papers-files/sam/mask_decoder_orient.png)
- 输入处理
  - src: 稠密化(image embedding) + mask embedding
  - pos_src: 稠密化(image pos)
  - tokens: iou_tokens + mask_tokens + sparse_prompt_embeddings(points embedding + box embedding)
- 个人理解
  - iou_tokens: [1, transformer_dim] 用来记录输出后每个mask的iou值
  - mask_tokens: [4, transformer_dim] 用来记录输出后每个mask的特征, 为实现ambiguity-aware
    - [dim 4] = 1(记录multimask_output=false) + 3(记录multimask_output=true)
  - 这两个tokens可以理解问NLP transformer中的[cls], 用了记录输出
- 输出结果
  - hs
    - hs提取出iou_token_out, mask_tokens_out
      - iou_token_out对应每个mask_tokens的iou score
      - mask_tokens_out与输出src做矩阵乘法, 得出输出mask, 这里的mask_tokens_out也是4维, 可以理解成做了多尺度的特征融合
  - src
    - 上采样4倍, 和mask_tokens_out融合后为输出mask

### Automatic Mask Generation
```py
# crop_box就是整张图的坐上和右下坐标
x0, y0, x1, y1 = crop_box
cropped_im = image[y0:y1, x0:x1, :]
cropped_im_size = cropped_im.shape[:2]
self.predictor.set_image(cropped_im)

# Get points for this crop
points_scale = np.array(cropped_im_size)[None, ::-1]
# self.point_grids就是a regular grid of 32×32 points on the full image, 即在整张图上撒了32x32个点
points_for_image = self.point_grids[crop_layer_idx] * points_scale
```